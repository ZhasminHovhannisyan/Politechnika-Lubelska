{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Determining optimal probability cut-off\n",
    "Predicted probability is a number between **0** and **1**. Traditionally, probability **>= 0.5** is a *cut-off point* used for converting predicted proabability to **1** (*positive*) otherwise **0** (*negative*). This logic works well when your training data set has equal example of positive and negative samples, however this is not the case in real world scenarios.\n",
    "\n",
    "The solution is to find the optimal *cut-off* point, that is the point where **true positive rate** is high and the **false positive rate** is low. Anything above this threshold can be labeled as **1** else **0**. Let's understand this better with an example.\n",
    "\n",
    "We'll be using the dataset from the **UCI repository**, dataset *Pima Indian diabetes*: 2 classes, 8 attributes, 768 instances, 500 (65.1%) negative (class1), and 268 (34.9%) positive tests for diabetes (class2). All patients were females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "Attributes used:\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml)\n",
    "6. Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)"
   ],
   "id": "6654e66bed0d1a8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 1: Read the input data",
   "id": "b8be7ec94cf0d352"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ],
   "id": "20688f34c1fab9a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read the data from the Diabetes.csv file\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Lab09/Diabetes.csv')\n",
    "print(df.head())\n",
    "print()\n",
    "\n",
    "# Target variable % distribution\n",
    "print (df['class'].value_counts(normalize = True))\n",
    "\n",
    "X = df.iloc[:, :8]    # Independent variables aaaa\n",
    "Y = df['class']       # Dependent variable"
   ],
   "id": "542f2a947f8942dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. What do the numbers **0** and **1** in the *class* column mean?\n",
    "2. In your opinion, is the dataset balanced or not? Why do you think so?\n",
    "\n",
    "**Answers:**"
   ],
   "id": "bea580a0e1dc3baf"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. In class column 0 means negative test results for diabetes(65.1042%) and 1 means positive result (34.8958%)\n",
    "2. The dataset is not balanced, because class 0 has 65% data and class 1 has only 35%. In a balanced dataset both classes should have nearly equal number of examples."
   ],
   "id": "cdfd3aa63fde1a1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 2: Model building and evaluation",
   "id": "51eb1033fc343ee0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)\n",
    "\n",
    "# Instantiate a logistic regression model, and fit\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model = model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict class labels for the train set\n",
    "# The predict() fuction converts probability values >= 0.5 to 1 else 0\n",
    "Y_pred = model.predict(X_train)\n",
    "print('Example results of the predict() function:', Y_pred[0:5])\n",
    "\n",
    "# Generate class probabilities with the predict_proba() function\n",
    "# Notice that 2 elements will be returned in probs array,\n",
    "# 1st element is a probability for negative class,\n",
    "# 2nd element gives probability for positive class\n",
    "probs = model.predict_proba(X_train)\n",
    "Y_pred_prob = probs[:, 1]\n",
    "print('\\nExample results of the predict_proba() function:\\n', probs[0:5, :])\n",
    "\n",
    "# Generate evaluation metrics\n",
    "print ('\\nAccuracy:', metrics.accuracy_score(Y_train, Y_pred))"
   ],
   "id": "8320db16f05d022a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. What is a difference between the results returned by the predict() and predict_proba() functions?\n",
    "2. How would you use these functions when solving a binary classification task?\n",
    "\n",
    "**Answers:**"
   ],
   "id": "ec33f2287eea0995"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. predict() applies a threshold 0.5 and returns the class label 0 or 1 for that instance. predict_proba() returns class probabilities for both classes (negative and positive)\n",
    "2. For final class prediction in binary classification predict() will be better option, but if we need to customise the threshold, plot ROC curves, AUC, understand the model's confidence in predictions or do other analysis, then predict_proba() is better option"
   ],
   "id": "8b2bcce1325884ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 3: Building the ROC curve",
   "id": "801059b95dafa23e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract false positive rate (fpr), true positive rate (tpr) and thresholds\n",
    "FPR, TPR, thresholds = metrics.roc_curve(Y_train, Y_pred_prob)\n",
    "ROC_AUC = metrics.auc(FPR, TPR)\n",
    "print ('Area under the ROC curve (AUC):', ROC_AUC)\n",
    "\n",
    "i = np.arange(len(TPR))    # Index for df\n",
    "ROC = pd.DataFrame({'FPR': pd.Series(FPR, index = i),\n",
    "                    'TPR': pd.Series(TPR, index = i),\n",
    "                    '1-FPR': pd.Series(1-FPR, index = i),\n",
    "                    'tf': pd.Series(TPR-(1-FPR), index = i),\n",
    "                    'thresholds': pd.Series(thresholds, index = i)})\n",
    "ROC.iloc[(ROC.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "# Plot tpr vs 1-fpr\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(ROC['TPR'], label = 'TPR', lw=3)\n",
    "plt.plot(ROC['1-FPR'], color = 'red', label = '1-FPR', lw=3)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel('1-FPR', fontsize=16)\n",
    "plt.ylabel('TPR', fontsize=16)\n",
    "#plt.title('Receiver operating characteristic')\n",
    "plt.show()"
   ],
   "id": "2583d33ea1cae33e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. What do the FPR and TPR parameters mean?\n",
    "2. What is the AUC parameter used for?\n",
    "\n",
    "**Answers:**"
   ],
   "id": "c871ddd531831226"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. FPR is false positive rate which shows the proportion of negative samples classified as positive. The lower the better.  TPR is true positive rate (also recall) which shows the proportion of positive samples correctly classified as positive. The higher the better.\n",
    "2. AUC is area under the ROC curve which measures model's abilitiy to differentiate the classes. AUC value 0.5 represents just random guessing, 1.0 is for perfect classifier and if AUC < 0.5 it means that model is misclassifying."
   ],
   "id": "a31aacb6a259a5cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 4: Optimal cut-off\n",
    "The optimal *cut-off* would be where **TPR** is high and **FPR** is low. In this case, **TPR-(1-FPR)** is **zero** or **near to zero**. To simplify finding **optimal probability threshold** and bring in re-usability, we may use a function to find the optimal probability **cut-off** point. The function used returns a list type with optimal **cut-off** value. The meaning of the parameters is as follows:\n",
    "- **target** - matrix with dependent or target data, where rows are observations;\n",
    "- **predicted** - matrix with predicted data, where rows are observations."
   ],
   "id": "1eb6be3b6c7d7da6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    FPR, TPR, threshold = metrics.roc_curve(target, predicted)\n",
    "    i = np.arange(len(TPR))\n",
    "    ROC = pd.DataFrame({'tf': pd.Series(TPR-(1-FPR), index = i), 'threshold': pd.Series(threshold, index = i)})\n",
    "    ROC_t = ROC.iloc[(ROC.tf-0).abs().argsort()[:1]]\n",
    "    return list(ROC_t['threshold'])"
   ],
   "id": "432a9e99cbd4dc89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find optimal probability threshold\n",
    "# Note: probs[:, 1] will have the probability of being a positive label\n",
    "threshold = Find_Optimal_Cutoff(Y_train, probs[:, 1])\n",
    "print ('Optimal probability threshold:', threshold)\n",
    "\n",
    "# Applying the threshold to the prediction probability\n",
    "Y_pred_optimal = np.where(Y_pred_prob >= threshold, 1, 0)\n",
    "\n",
    "# Let's compare the accuracy of traditional/normal approach vs. optimal cut-off\n",
    "print ('\\nAccuracy (normal):', metrics.accuracy_score(Y_train, Y_pred))\n",
    "print ('Accuracy (optimal cut-off):', metrics.accuracy_score(Y_train, Y_pred_optimal))\n",
    "print ('\\nConfusion matrix (normal):\\n', metrics.confusion_matrix(Y_train, Y_pred))\n",
    "print ('\\nConfusion matrix (optimal cut-off): \\n', metrics.confusion_matrix(Y_train, Y_pred_optimal))"
   ],
   "id": "2a8b18a7ad704636"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. How did type I and type II errors change after the application of the optimal cut-off?\n",
    "2. Do you think this change is medically beneficial for patients? Justify your answer.\n",
    "\n",
    "**Answers:**"
   ],
   "id": "516cb7a40d782b92"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. After the application of optimal cut-off type 1 error (False Positives) increased from 39 to 83. And type 2 (False negative) error decreased from 72 to 46.\n",
    "2. The model is giving less false negative results, which means more patients with diabetes are diagnosed as having diabetes and medically this is benefitial for patients. However, there will be cases that patient not having diabetes will be classified as having one, but the consequences are not severe here since they can undergo another examination and discover that they don't have diabetes."
   ],
   "id": "4a5b239f03c1bbe1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Rare event or unbalanced data set\n",
    "Providing equal sample of positive and negative instances to the classification algorithm will result in optimal result. Dataset which are highly skewed towards one or more classes have proven to be a challenge.\n",
    "\n",
    "Resampling is a common practice of addressing this issue. Although there are many techniques within re-sampling, here we'll be learning the 3 most popular techniques.\n",
    "\n",
    "* **Random under-sampling** - reduce majority class to match minority class count.\n",
    "* **Random over-sampling** - increase minority class by randomly picking samples within minority class till counts of both class match.\n",
    "* **Synthetic Minority Over Sampling Technique (SMOTE)** - increase minority class by introducing synthetic examples through connecting all k (default = 5) minority class nearest negihbors using feature space similarity (Euclidean distance)."
   ],
   "id": "bf3cc233c830119e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 5: Generate the input data\n",
    "A number of techniques have been implemented in **imbalanced-learn** python package. To install it, close Jupyter Notebook and enter the command in the console: **pip install -U imbalanced-learn**. Then, open Jupyter Notebook and continue with the exercises."
   ],
   "id": "bc0648ea03a246b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ],
   "id": "654435fc5896b84d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate the dataset with 2 features to keep it simple\n",
    "X, Y = make_classification(n_samples = 1000,\n",
    "                           n_features = 2,\n",
    "                           n_informative = 2,\n",
    "                           n_redundant = 0,\n",
    "                           weights = [0.8, 0.2],\n",
    "                           random_state = 666)\n",
    "\n",
    "print ('Negative class:', Y.tolist().count(0))\n",
    "print ('Positive class:', Y.tolist().count(1))"
   ],
   "id": "298305dd640592bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Question:**\n",
    "1. On what basis can it be concluded that the data are unbalanced?\n",
    "\n",
    "**Answer:**"
   ],
   "id": "f07296abdd5d0919"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "The data are unbalanced because the class distribution is skewed towards negative class, which is nearly 4 times more than positive class",
   "id": "fd112565e1def3ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 6: Applying the sampling techniques and plot the original vs. re-sampled",
   "id": "671534f43f595a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Apply the Random Under-Sampling\n",
    "RUS = RandomUnderSampler()\n",
    "X_RUS, Y_RUS = RUS.fit_resample(X, Y)\n",
    "\n",
    "# Apply the Random Over-Sampling\n",
    "ROS = RandomOverSampler()\n",
    "X_ROS, Y_ROS = ROS.fit_resample(X, Y)\n",
    "\n",
    "# Apply regular SMOTE\n",
    "SM = SMOTE()\n",
    "X_SMOTE, Y_SMOTE = SM.fit_resample(X, Y)"
   ],
   "id": "117f2f2de2c69d0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Original vs resampled subplots\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(X[Y == 0, 0], X[Y == 0, 1], marker = '+', color = 'blue')\n",
    "plt.scatter(X[Y == 1, 0], X[Y == 1, 1], marker='+', color = 'red')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Original: 1=%s and 0=%s' %(Y.tolist().count(1), Y.tolist().count(0)))\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(X_RUS[Y_RUS == 0, 0], X_RUS[Y_RUS == 0, 1], marker = '+', color = 'blue')\n",
    "plt.scatter(X_RUS[Y_RUS == 1, 0], X_RUS[Y_RUS == 1, 1], marker = '+', color = 'red')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y2')\n",
    "plt.title('Random Under-Sampling: 1=%s and 0=%s' %(Y_RUS.tolist().count(1), Y_RUS.tolist().count(0)))\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(X_ROS[Y_ROS == 0, 0], X_ROS[Y_ROS == 0, 1], marker = '+', color = 'blue')\n",
    "plt.scatter(X_ROS[Y_ROS == 1, 0], X_ROS[Y_ROS == 1, 1], marker = '+', color = 'red')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Random Over-Sampling: 1=%s and 0=%s' %(Y_ROS.tolist().count(1), Y_ROS.tolist().count(0)))\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(X_SMOTE[Y_SMOTE == 0, 0], X_SMOTE[Y_SMOTE == 0, 1], marker = '+', color = 'blue')\n",
    "plt.scatter(X_SMOTE[Y_SMOTE == 1, 0], X_SMOTE[Y_SMOTE == 1, 1], marker = '+', color = 'red')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y2')\n",
    "plt.title('SMOTE: 1=%s and 0=%s' %(Y_SMOTE.tolist().count(1), Y_SMOTE.tolist().count(0)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c7286e812535854c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. What is in your opinion the disadvantage of the Random Under-Sampling technique?\n",
    "2. What are in your opinion the disadvantages of the Random Over-Sampling and SMOTE techniques?\n",
    "\n",
    "**Answers:**"
   ],
   "id": "91d4d022f2be72db"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. Random under-sampling raises the opportunity for loss of information or concepts, because we are reducing the majority class.\n",
    "2. Random over-sampling and SMOTE can lead to overfitting because of the multiple related instances."
   ],
   "id": "48fe922e5fe0ce3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 7: Comparison of effectiveness of the re-sampling techniques\n",
    "We'll build classification models for 3 re-sampled data and compare their accuracy using AUC."
   ],
   "id": "571b8631f93b03c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_RUS_train, X_RUS_test, Y_RUS_train, Y_RUS_test = train_test_split(X_RUS, Y_RUS, test_size = 0.3)\n",
    "X_ROS_train, X_ROS_test, Y_ROS_train, Y_ROS_test = train_test_split(X_ROS, Y_ROS, test_size = 0.3)\n",
    "X_SMOTE_train, X_SMOTE_test, Y_SMOTE_train, Y_SMOTE_test = train_test_split(X_SMOTE, Y_SMOTE, test_size = 0.3)\n",
    "\n",
    "# Build a decision tree classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf_rus = clf.fit(X_RUS_train, Y_RUS_train)\n",
    "clf_ros = clf.fit(X_ROS_train, Y_ROS_train)\n",
    "clf_smote = clf.fit(X_SMOTE_train, Y_SMOTE_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "print ('RUS - Train AUC:', metrics.roc_auc_score(Y_RUS_train, clf.predict(X_RUS_train)))\n",
    "print ('RUS - Test AUC:', metrics.roc_auc_score(Y_RUS_test, clf.predict(X_RUS_test)))\n",
    "print ('\\nROS - Train AUC:', metrics.roc_auc_score(Y_ROS_train, clf.predict(X_ROS_train)))\n",
    "print ('ROS - Test AUC:', metrics.roc_auc_score(Y_ROS_test, clf.predict(X_ROS_test)))\n",
    "print ('\\nSMOTE - Train AUC:', metrics.roc_auc_score(Y_SMOTE_train, clf.predict(X_SMOTE_train)))\n",
    "print ('SMOTE - Test AUC:', metrics.roc_auc_score(Y_SMOTE_test, clf.predict(X_SMOTE_test)))"
   ],
   "id": "f9ece10a6f5afaed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. Which re-sampling technique did you find most effective? Justify your answer.\n",
    "2. What other ways of assessing model quality could you recommend?\n",
    "\n",
    "**Answers:**"
   ],
   "id": "e737cb9af0b6c9d5"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. The most effective method is Random Over-Sampling (ROS), because it has the highest test AUC 0.98, which means it has a good generalization and predicts well the new data. Also in case of ROS the train and test AUC are the closest to eachother, which indicates less overfitting than other two methods.\n",
    "2. Other ways of assessing model quality are for example confusion matrix (comparing FP-s and FN-s). Or with F1 score, which balances precision and recall."
   ],
   "id": "6f711c4248c27943"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. K-fold cross validation\n",
    "**K-fold** cross-validation splits the training dataset into **k** folds without replacement i.e., any given data point will only be part of one of the subset, where **k-1** folds are used for the model training and **one** fold is used for testing. The procedure is repeated **k** times so that we obtain **k** models and performance estimates.\n",
    "\n",
    "We then calcuate the averge performance of the models based on the individual folds to obtain a performance estimate that is less sensitive to the subpartitioning of the training data compared to the **hold-out** or **single fold** method.\n",
    "\n",
    "An extended cross-validation is the **stratified k-fold** cross validation, where the class proportions are preserved in each fold leading to a better **bias** and **variance** estimates."
   ],
   "id": "34426828977a252d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 8: Hold-out or single fold model",
   "id": "2aed7b7b2f0b92d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Lab09/Diabetes.csv')\n",
    "\n",
    "X = df.iloc[:,:8].values    # Independent variables\n",
    "y = df['class'].values      # Dependent variable\n",
    "\n",
    "# Normalize data\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X = sc.transform(X)\n",
    "\n",
    "# Evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Instantiate a logistic regression model, and fit\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "print ('Train score:', clf.score(X_train, y_train))\n",
    "print ('Test score:', clf.score(X_test, y_test))"
   ],
   "id": "d809ef2924eaa37a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 9: 5-fold cross-validation",
   "id": "be974d1df6532a29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate the model using 5-fold cross-validation\n",
    "train_scores = cross_val_score(clf, X_train, y_train, scoring = 'accuracy', cv = 5)\n",
    "test_scores = cross_val_score(clf, X_test, y_test, scoring = 'accuracy', cv = 5)\n",
    "print ('Train fold AUC scores:', train_scores)\n",
    "print ('Train CV AUC score:', train_scores.mean())\n",
    "\n",
    "print ('\\nTest fold AUC scores:', test_scores)\n",
    "print ('Test CV AUC score:', test_scores.mean())"
   ],
   "id": "54e5b73c64ce9cf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. Explain, how 5-fold cross-validation is performed?\n",
    "2. Run the code of exercises 8 and 9 five times and note the results. Compare the results obtained with the single fold model and the 5-fold cross-validation.\n",
    "\n",
    "**Answers:**"
   ],
   "id": "1a65f738ff91ec1d"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. 5-fold cross validation splits the training dataset into 5 folds without replacement. Any given datapoint will be part of only one subset and 4 folds will be used to train the model, then the 5ts one will be used to testing. The procedure is repeated 5 times. After 5 iteraions, the performance metrics are averaged to get the final estimate of model's performance.\n",
    "\n",
    "2. For the single-fold model in the result of running the code five times the train score ranges from 0.780 to 0.795, test score is in between 0.727 and 0.787. This method depends on the train-test split, that's why it is not consistent and sensitive. In some cases this may lead to potential bias or have a risk of overfitting. For the 5-fold model the averaged scores of CV AUC scores show less sensitivity, they are more stable and reliable.  The results of running the codes five times is shown below.\n",
    "\n",
    "**Single-fold model:**\n",
    "\n",
    "*First result:* Train score - 0.7951582867783985,\n",
    "Test score - 0.7402597402597403.\n",
    "\n",
    "*Second result:* Train score - 0.7839851024208566, Test score - 0.7619047619047619.\n",
    "\n",
    "*Third result:* Train score - 0.7895716945996276, Test score - 0.7272727272727273.\n",
    "\n",
    "*Fourth result:* Train score - 0.7802607076350093, Test score - 0.7878787878787878.\n",
    "\n",
    "*Fifth result:* Train score - 0.7839851024208566, Test score - 0.7489177489177489.\n",
    "\n",
    "**5-fold model:**\n",
    "\n",
    "*First result:* Train fold AUC scores: [0.80555556 0.7962963  0.71028037 0.82242991 0.76635514]\n",
    "Train CV AUC score: 0.7801834544825199\n",
    "Test fold AUC scores: [0.65957447 0.7173913  0.76086957 0.7826087  0.7826087 ]\n",
    "Test CV AUC score: 0.7406105457909342\n",
    "\n",
    "\n",
    "*Second result:* Train fold AUC scores: [0.74074074 0.77777778 0.73831776 0.81308411 0.76635514]\n",
    "Train CV AUC score: 0.7672551055728626\n",
    "Test fold AUC scores: [0.76595745 0.82608696 0.67391304 0.73913043 0.82608696]\n",
    "Test CV AUC score: 0.7662349676225716\n",
    "\n",
    "\n",
    "*Third result:* Train fold AUC scores: [0.78703704 0.75       0.80373832 0.78504673 0.81308411]\n",
    "Train CV AUC score: 0.7877812391831084\n",
    "Test fold AUC scores: [0.76595745 0.69565217 0.84782609 0.80434783 0.60869565]\n",
    "Test CV AUC score: 0.7444958371877891\n",
    "\n",
    "\n",
    "*Fourth result:* Train fold AUC scores: [0.75925926 0.85185185 0.74766355 0.73831776 0.76635514]\n",
    "Train CV AUC score: 0.7726895119418484\n",
    "Test fold AUC scores: [0.80851064 0.69565217 0.67391304 0.82608696 0.80434783]\n",
    "Test CV AUC score: 0.7617021276595745\n",
    "\n",
    "\n",
    "*Fifth result:* Train fold AUC scores: [0.75       0.69444444 0.82242991 0.81308411 0.77570093]\n",
    "Train CV AUC score: 0.7711318795430945\n",
    "Test fold AUC scores: [0.72340426 0.76086957 0.7173913  0.73913043 0.80434783]\n",
    "Test CV AUC score: 0.7490286771507864"
   ],
   "id": "213271b5f7cc767"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 10: Stratified 5-fold cross-validation",
   "id": "643d71ee730adad8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "kfold = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "k = 0\n",
    "for (train, test) in kfold.split(X_train, y_train):\n",
    "    clf.fit(X_train[train], y_train[train])\n",
    "    train_score = clf.score(X_train[train], y_train[train])\n",
    "    train_scores.append(train_score)\n",
    "    # Score for test set\n",
    "    test_score = clf.score(X_train[test], y_train[test])\n",
    "    test_scores.append(test_score)\n",
    "\n",
    "    k += 1\n",
    "    print('Fold: %s, Class distribution: %s, Train Acc: %.3f, Test Acc: %.3f'\n",
    "          % (k, np.bincount(y_train[train]), train_score, test_score))\n",
    "\n",
    "print('\\nTrain CV accuracy: %.3f' % (np.mean(train_scores)))\n",
    "print('Test CV accuracy: %.3f' % (np.mean(test_scores)))"
   ],
   "id": "e3c379537269bc60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. Explain, how stratified 5-fold cross-validation is performed?\n",
    "2. Compare the results obtained with the 5-fold cross-validation and the stratified 5-fold cross-validation.\n",
    "\n",
    "**Answers:**"
   ],
   "id": "ffaff33df32cdfdf"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. Stratified K-Fold Cross-Validation is an extention of K-Fold model, which maintains the same class proportion for each fold as it is for the original dataset. This is being useful for imbalanced datasets.\n",
    "2. Stratified Cross-validation gives higher test CV accuracy (0.771) compared to regular 5-fold cv (maximum 0.766 out of five results in the ex 9). This is because the class proportions are kept the same, which improved the generalization."
   ],
   "id": "6bcfa29889cfa798"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 11: Plotting the ROC curve for the stratified KFold",
   "id": "e0f74da3f58883b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "from numpy import interp\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "colors = cycle(['darkorange', 'indigo', 'seagreen', 'magenta', 'blue'])\n",
    "lw = 2\n",
    "\n",
    "i = 0\n",
    "for (train, test), color in zip(kfold.split(X, y), colors):\n",
    "    probas_ = clf.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    # Compute the ROC curve and the area under the curve (AUC)\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw = lw, color = color, label = 'ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle = '--', lw = lw, color = 'k', label = 'Random Choice')\n",
    "\n",
    "mean_tpr /= kfold.get_n_splits(X, y)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color = 'r', linestyle = '--', label = 'Mean ROC (area = %0.2f)' % mean_auc, lw = lw)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ],
   "id": "8827ed2095d9decc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. In what situation would you recommend using the AUC index to assess the model quality?\n",
    "2. Which of the curves shown in the plot corresponds to the best model? Justify your answer?\n",
    "3. What value of the AUC parameter should be taken as an index of the model quality? Why do you think so?\n",
    "\n",
    "**Answers:**"
   ],
   "id": "433557dd1731f864"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. The AUC is useful for imbalanced datasets, because it measures the model's ability to differentiate between negative and positive classes. It can also be useful for binary classification problems to show how well classifier separates the two classes. And since AUC is independent of the decision threshold it can be useful for threshold-independent evaluation.\n",
    "2. From the plot ROC fold 3 (magenda) is the best model, AUC = 0.87, because the higher AUC shows model's ability to better separate positive and negative classes.\n",
    "3. Index of model quality can be taken AUC = 0.84, but the best one is 0.87"
   ],
   "id": "f38e743ca3d77202"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Bagging\n",
    "**Bootstrap aggregation** (also known as **bagging**) is a model aggregation technique to reduce model variance. The training data is splited into multiple samples with replacement called as **boostrap samples**. Bootstrap sample size is same as original sample size, with **3/4th** of the original values and replacement result in repetition of values."
   ],
   "id": "700cccc55d1ed175"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 12: 5-fold cross validation and bagging",
   "id": "2bcd294cccf205eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bagged decision trees for Classification\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Read the data in\n",
    "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Lab09/Diabetes.csv')\n",
    "\n",
    "X = df.iloc[:,:8].values    # Independent variables\n",
    "y = df['class'].values      # Dependent variable\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "#kfold = model_selection.StratifiedKFold(n_splits = 5)\n",
    "#kfold = model_selection.StratifiedKFold(n_splits = 7)\n",
    "kfold = model_selection.StratifiedKFold(n_splits = 9)\n",
    "num_trees = 100\n",
    "\n",
    "# Dection tree with 5-fold cross validation\n",
    "clf_DT = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_DT, X_train, y_train, cv = kfold)\n",
    "print ('Decision tree (stand alone) - train:', results.mean())\n",
    "print ('Decision tree (stand alone) - test:', metrics.accuracy_score(clf_DT.predict(X_test), y_test))\n",
    "\n",
    "# Using bagging lets build 100 decision tree models and average / majority vote prediction\n",
    "clf_DT_Bag = BaggingClassifier(estimator = clf_DT, n_estimators = num_trees).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_DT_Bag, X_train, y_train, cv = kfold)\n",
    "print ('\\nDecision tree (bagging) - train:', results.mean())\n",
    "print ('Decision tree (bagging) - test:', metrics.accuracy_score(clf_DT_Bag.predict(X_test), y_test))"
   ],
   "id": "62f1488594877fe2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. Which method lets us to build a more accurate model? Justify your answer.\n",
    "2. Could another way of splitting the data improve the results? Check this for 7-fold and 9-fold cross validation.\n",
    "\n",
    "**Answers:**"
   ],
   "id": "fcbf5fe84a43b21d"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. For k = 5 the results are as follows:\n",
    "stand alone desicion tree - train: 0.689, test: 0.779, bagging desicion tree  - train: 0.769,  test: 0.747.\n",
    "If to compare only with test accuracy, then standalone desicion tree makes more accurate desicions, however bagging improves the training accuracy.\n",
    "2. k = 9 gives the best performance, it has the highest test accuracy for both standalone desicion tree and bagging desicion tree. Using k = 9 will help to generalize model and reduce overfitting. Results are below.\n",
    "\n",
    "Results for k = 7: stand alone desicion tree - train: 0.723, test: 0.688, bagging desicion tree  - train: 0.762, test: 0.753.\n",
    "\n",
    "Results for k = 9: stand alone desicion tree - train: 0.720, test: 0.714, bagging desicion tree  - train: 0.754,  test: 0.779.\n"
   ],
   "id": "521ab0e3996b62cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 13: Feature importances\n",
    "Decision tree model has an attribute to show important features which is based on the **gini** or **entropy information gain**."
   ],
   "id": "695cb304dc40aa52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "feature_importance = clf_DT.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align = 'center')\n",
    "plt.yticks(pos, df.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ],
   "id": "f84a3985d9e1dec2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Question:**\n",
    "1. How do you think an information about the feature importances can be used?\n",
    "\n",
    "**Answer:**"
   ],
   "id": "7b5b277cc13158af"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "Information about feature importance helps to simplify the model using only most important features, which can help to avoid overfitting, because less important features may add noise and decrease generalisation.",
   "id": "3612bf8e63aab37f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 14: Random forest\n",
    "A **subset of observations** and a **subset of vairables** are randomly picked to build multiple independent **tree** based models. The trees are more un-correlated as only a subset of variables are used during the split of the tree, rather than greedily choosing the best split point in the construction of the tree."
   ],
   "id": "6857ac9788dea4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "num_trees = 100\n",
    "# num_trees = 150\n",
    "# num_trees = 300\n",
    "# num_trees = 450\n",
    "\n",
    "X = df.iloc[:,:8].values    # Independent variables\n",
    "y = df['class'].values      # Dependent variable\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "\n",
    "\n",
    "clf_RF = RandomForestClassifier(n_estimators = num_trees).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_RF, X_train, y_train, cv = kfold)\n",
    "\n",
    "print ('Random forest - train:', results.mean())\n",
    "print ('Random forest - test:', metrics.accuracy_score(clf_RF.predict(X_test), y_test))"
   ],
   "id": "6c4ee49eed19d4b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. Is a model built using the **random forest** method better than a model built using the **bagging** method? Justify your answer.\n",
    "2. Run the **random forest** model building process 5 times and note the accuracy of training and testing each time. How do you justify the different values of the results obtained?\n",
    "3. How does the number of independent trees used to build a **random forest** model affect its effectiveness? Check this for several values of the **num_trees** parameter.\n",
    "\n",
    "**Answers:**"
   ],
   "id": "b30fae0d90ff7b7d"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. They both have the same testing accuracy 0.747, but bagging desicion tree has higher training accuracy 0.769 than random forest 0.754. So the bagging decision tree is slighly better.\n",
    "2. The reason for different accuracies is bootstraping. Each tree in the forest is trained on different subset of data and the data is split differently every time we run the programm. The accuracy differs also because of feature selection. Every time we run the programm, each tree is given random subset of features, which leads to variation in performance. Results are below\n",
    "*First result:* train: 0.7540050646408103, test: 0.7467532467532467;\n",
    "\n",
    "*Second result:* train: 0.778381980541117, test: 0.7142857142857143;\n",
    "\n",
    "*Third result:* train: 0.7459149673463947, test: 0.7662337662337663;\n",
    "\n",
    "*Fourth result:* train: 0.7736238837798214, test: 0.7402597402597403;\n",
    "\n",
    "*Fifth result:* train: 0.7638411302145809, test: 0.7922077922077922\n",
    "3. With the increase of num_trees, training accuracy of model slightly increases. However, after a certain point the increase in training accuracy stops, which shows that adding more trees will not always lead to better performance. Results are below\n",
    "*The output for num_trees = 100:* train: 0.7540050646408103, test: 0.7467532467532467;\n",
    "\n",
    "*The output for num_trees = 150:*  train: 0.7525389844062376, test: 0.7662337662337663;\n",
    "\n",
    "*The output for num_trees = 300:*  train: 0.767119818739171, test: 0.7467532467532467;\n",
    "\n",
    "*The output for num_trees = 450:*  train: 0.7703052112488338, test: 0.7662337662337663;\n"
   ],
   "id": "f7f3335743887f33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 15: Extremely randomized trees\n",
    "This algorithm is an effort to introduce more randomness to the **bagging** process. Tree splits are chosen completely at random from the range of values in the sample at each split which allows to *reduce the variance of the model* further, however at the cost of *slight increase in bias*."
   ],
   "id": "7338ed613dfe1458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "num_trees = 100\n",
    "\n",
    "X = df.iloc[:,:8].values    # Independent variables\n",
    "y = df['class'].values      # Dependent variable\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "clf_ET = ExtraTreesClassifier(n_estimators = num_trees).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_ET, X_train, y_train, cv = kfold)\n",
    "\n",
    "print ('ExtraTree - train:', results.mean())\n",
    "print ('ExtraTree - test:', metrics.accuracy_score(clf_ET.predict(X_test), y_test))"
   ],
   "id": "60269a00a53bd350"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. Compare the average accuracy of the **extremely randomized trees** and **random forest** models for **10 trials**. How do you justify your results?\n",
    "2. Do you think the **test_size** parameter affects the accuracy of model testing? Justify your answer.\n",
    "\n",
    "**Answers:**"
   ],
   "id": "4ce5882b1f9bac9f"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. results\n",
    "*First:* ExtraTree - train: 0.7556977209116353,\n",
    "ExtraTree - test: 0.7402597402597403;\n",
    "Random forest - train: 0.7606157536985206,\n",
    "Random forest - test: 0.7532467532467533;\n",
    "\n",
    "*Second:* ExtraTree - train: 0.7328135412501666,\n",
    "ExtraTree - test: 0.7922077922077922;\n",
    "Random forest - train: 0.773610555777689,\n",
    "Random forest - test: 0.7272727272727273;\n",
    "\n",
    "*Third:* ExtraTree - train: 0.7540583766493403,\n",
    "ExtraTree - test: 0.7597402597402597;\n",
    "Random forest - train: 0.7703851792616286,\n",
    "Random forest - test: 0.6948051948051948;\n",
    "\n",
    "*Fourth:* ExtraTree - train: 0.7638677862188459,\n",
    "ExtraTree - test: 0.7727272727272727;\n",
    "Random forest - train: 0.760682393709183,\n",
    "Random forest - test: 0.7597402597402597;\n",
    "\n",
    "*Fifth:* ExtraTree - train: 0.7605624416899908,\n",
    "ExtraTree - test: 0.7402597402597403;\n",
    "Random forest - train: 0.7589097694255631,\n",
    "Random forest - test: 0.7857142857142857;\n",
    "\n",
    "*Sixth:* ExtraTree - train: 0.742729574836732,\n",
    "ExtraTree - test: 0.7662337662337663;\n",
    "Random forest - train: 0.7638144742103159,\n",
    "Random forest - test: 0.7467532467532467;\n",
    "\n",
    "*Seventh:* ExtraTree - train: 0.7523923763827802,\n",
    "ExtraTree - test: 0.7922077922077922;\n",
    "Random forest - train: 0.7753431960549113,\n",
    "Random forest - test: 0.7922077922077922;\n",
    "\n",
    "*Eight:* ExtraTree - train: 0.7574170331867254,\n",
    "ExtraTree - test: 0.7337662337662337;\n",
    "Random forest - train: 0.7606157536985206,\n",
    "Random forest - test: 0.7922077922077922;\n",
    "\n",
    "*Ninth:* ExtraTree - train: 0.7442889510862323,\n",
    "ExtraTree - test: 0.7662337662337663;\n",
    "Random forest - train: 0.7572837531654004,\n",
    "Random forest - test: 0.7272727272727273;\n",
    "\n",
    "*Tenth:* ExtraTree - train: 0.75083300013328,\n",
    "ExtraTree - test: 0.7337662337662337;\n",
    "Random forest - train: 0.760682393709183,\n",
    "Random forest - test: 0.7597402597402597;\n",
    "\n",
    "Based on this results for extremely randomized trees the average train accuracy will be 0.751 and the avg test accuracy will be 0.760.  For random forest the avg train will be 0.764 and the avg test will be 0.754. So random forest has slightly higher training accuracy compared to ExtraTree and ExtraTree has higher test accuracy.\n",
    "\n",
    "2. Yes, the test_size parameter affects the accuracy of model testing. The test_size determines the proportion of dataset taken for training and testing, which affects model performance."
   ],
   "id": "fe52f2af24561f01"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
