{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Boosting\n",
    "In 1995. Freud and Schapire introduced the concept of **boosting** with the well know **AdaBoost** algorithm. The core concept of boosting is that rather than a independent individual hypothesis, combining hypothesis in a sequential order increases the accuracy. Essentially, boosting algorithms convert the *weak learners* into *strong learners*. Boosting algorithms are well designed to address the **bias** problems.\n",
    "## 1.1. AdaBoosting\n",
    "## Exercise 1: AdaBoosting"
   ],
   "id": "aad486d55f73531e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bagged decision trees for classification\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Read the data in\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Lab10/Diabetes.csv')\n",
    "\n",
    "# Let's use some weak features to build the tree\n",
    "X = df[['age','serum_insulin']]    # Independent variables\n",
    "y = df['class'].values             # Dependent variable\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(n_splits = 5)\n",
    "num_trees = 100\n",
    "\n",
    "# Decision tree with 5-fold cross validation\n",
    "# Lets restrict max_depth to 1 to have more impure leaves\n",
    "clf_DT = DecisionTreeClassifier(max_depth = 1).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_DT, X_train, y_train, cv = kfold.split(X_train, y_train))\n",
    "print('Decision tree (stand alone) - CV train: %.2f' % results.mean())\n",
    "print('Decision tree (stand alone) - train: %.2f' % metrics.accuracy_score(clf_DT.predict(X_train), y_train))\n",
    "print('Decision tree (stand alone) - test: %.2f' % metrics.accuracy_score(clf_DT.predict(X_test), y_test))\n",
    "\n",
    "# Using adaptive boosting of 100 iteration\n",
    "clf_DT_Boost = AdaBoostClassifier(estimator = clf_DT, n_estimators = num_trees, learning_rate = 0.1).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_DT_Boost, X_train, y_train, cv = kfold)\n",
    "print('\\nDecision tree (AdaBoosting) - CV train: %.2f' % results.mean())\n",
    "print('Decision tree (AdaBoosting) - train: %.2f' % metrics.accuracy_score(clf_DT_Boost.predict(X_train), y_train))\n",
    "print('Decision tree (AdaBoosting) - test: %.2f' % metrics.accuracy_score(clf_DT_Boost.predict(X_test), y_test))"
   ],
   "id": "425a64c88a537a69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. Explain, how adaptive boosting is performed?\n",
    "2. Execute the above code several times and compare the results obtained using the **dection tree with 5-fold cross validation** model and the **adaptive boosting** model.\n",
    "\n",
    "**Answers:**"
   ],
   "id": "17ca57a44806762e"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. AdaBoost improves performance by combining multiple weak learners in a sequential manner. It's done in three steps: First assigning weights to all data points. Second at each iteration fit the classifier to the training data and update weights to minimize the weighted error rate. And at the end by majority votiing is selected the predicted class.\n",
    "2. Standalone desicion tree has lower accuracy compared to AdaBoost, because AdaBoost reduces bias and also in Standalone desicion tree we have max_depth = 1, which is very simple case to have good results.\n",
    "Results below:\n",
    "\n",
    "First execution:\n",
    "Decision tree (stand alone) - CV train: 0.64;\n",
    "Decision tree (stand alone) - train: 0.65;\n",
    "Decision tree (stand alone) - test: 0.66;\n",
    "Decision tree (AdaBoosting) - CV train: 0.69;\n",
    "Decision tree (AdaBoosting) - train: 0.71;\n",
    "Decision tree (AdaBoosting) - test: 0.70;\n",
    "\n",
    "Second exesution:\n",
    "Decision tree (stand alone) - CV train: 0.64;\n",
    "Decision tree (stand alone) - train: 0.66;\n",
    "Decision tree (stand alone) - test: 0.61;\n",
    "Decision tree (AdaBoosting) - CV train: 0.69;\n",
    "Decision tree (AdaBoosting) - train: 0.71;\n",
    "Decision tree (AdaBoosting) - test: 0.66;\n",
    "\n",
    "Third execution:\n",
    "Decision tree (stand alone) - CV train: 0.63;\n",
    "Decision tree (stand alone) - train: 0.66;\n",
    "Decision tree (stand alone) - test: 0.63;\n",
    "Decision tree (AdaBoosting) - CV train: 0.69;\n",
    "Decision tree (AdaBoosting) - train: 0.70;\n",
    "Decision tree (AdaBoosting) - test: 0.68;\n",
    "\n",
    "Fourth execution:\n",
    "Decision tree (stand alone) - CV train: 0.65;\n",
    "Decision tree (stand alone) - train: 0.66;\n",
    "Decision tree (stand alone) - test: 0.63;\n",
    "Decision tree (AdaBoosting) - CV train: 0.68;\n",
    "Decision tree (AdaBoosting) - train: 0.70;\n",
    "Decision tree (AdaBoosting) - test: 0.71;"
   ],
   "id": "d449c448af121b62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.2. Gradient boosting\n",
    "Due to the stage wise addictivity, the loss function can be represented in a form suitable for optimization. This gave birth to a class of generalized boosting algorithms known as **generalized boosting** algorithm (**GBM**).\n",
    "\n",
    "**Gradient boosting** is an example implementation of **GBM** and it can work with different loss functions such as regression, classification, risk modelling etc. As the name suggested it is a boosting algorithm which identifies shortcomings of a *weak learner* by **gradients** (**AdaBoost** uses high-weight data points), hence the name **gradient boosting**.\n",
    "## Exercise 2: Gradient boosting"
   ],
   "id": "2d876e8d479cc3e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "num_trees = 100\n",
    "# Using gradient boosting of 100 iterations\n",
    "clf_GBT = GradientBoostingClassifier(n_estimators = num_trees, learning_rate = 0.1).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_GBT, X_train, y_train, cv = kfold)\n",
    "\n",
    "print ('Gradient boosting - CV train: %.2f' % results.mean())\n",
    "print ('Gradient boosting - train: %.2f' % metrics.accuracy_score(clf_GBT.predict(X_train), y_train))\n",
    "print ('Gradient boosting - test: %.2f' % metrics.accuracy_score(clf_GBT.predict(X_test), y_test))"
   ],
   "id": "4c0630e7a3bee3e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. How does the number of independent trees used to build a **gradient boosting** model affect its effectiveness? Check this for a various number of trees.\n",
    "2. Execute the above code several times and compare the results obtained using the **adaptive boosting** and **gradient boosting** models.\n",
    "\n",
    "**Answers:**"
   ],
   "id": "82fb82bb1035907"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. Number of independent trees is n_estimators, which shows the complexity of model. Gradient boosting builds trees sequentially, where each next tree is correcting the errord of the previous tree by minimizing loss function with gradients. The number should be chosen carefully, beacuse as can be seen in results (results are below), increasing the number of trees will lead to overfitting. But desreasing too much (e.g. 50) will cause underfitting. Both of them result in small accuracy.\n",
    "Results:\n",
    "\n",
    "n_estimators = 50\n",
    "\n",
    "Gradient boosting - CV train: 0.66;\n",
    "Gradient boosting - train: 0.78;\n",
    "Gradient boosting - test: 0.68;\n",
    "\n",
    "\n",
    "n_estimators = 100\n",
    "\n",
    "Gradient boosting - CV train: 0.67;\n",
    "Gradient boosting - train: 0.80;\n",
    "Gradient boosting - test: 0.68;\n",
    "\n",
    "\n",
    "n_estimators = 150\n",
    "\n",
    "Gradient boosting - CV train: 0.69;\n",
    "Gradient boosting - train: 0.83;\n",
    "Gradient boosting - test: 0.66;\n",
    "\n",
    "\n",
    "n_estimators = 300\n",
    "\n",
    "Gradient boosting - CV train: 0.67;\n",
    "Gradient boosting - train: 0.85;\n",
    "Gradient boosting - test: 0.66;\n",
    "\n",
    "\n",
    "2. Cross-validation train accuracy is close to eachother for both (0.67-0.69). Gradient boosting has higher training accuracy (0.78-0.85) than Adaboost (0.70-0.72).\n"
   ],
   "id": "142935f536aa6030"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.3. XGBoost (eXtreme Gradient Boosting)\n",
    "In March 2014, Tianqui Chen built **xgboost** in C++ as part of Distributed (Deep) Machine Learning Community that has interface for Python. It is an extended, more regularized version of **gradient boosting** algorithm. This is one of the most well performing large-scale, scalable machine learning algorithms which has been playing a major role in winning solutions of Kaggle (forum for predictive modelling and analytics competition) data science competition. sklearn has a wrapper for **xgboost** (**XGBClassifier**).\n",
    "## Exercise 3: XGBoost"
   ],
   "id": "99247e06e856d06d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install --upgrade xgboost",
   "id": "ec53f7aa8dd9564f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "\n",
    "# Read the data in\n",
    "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Lab10/Diabetes.csv')\n",
    "\n",
    "predictors = ['age','serum_insulin']\n",
    "target = 'class'\n",
    "\n",
    "df.fillna((-999), inplace = True)\n",
    "\n",
    "# Let's use some weak features to build the tree\n",
    "X = df[['age','serum_insulin']]    # Independent variables\n",
    "y = df['class'].values             # Dependent variable\n",
    "\n",
    "# Evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "num_rounds = 100\n",
    "kfold = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "clf_XGB = XGBClassifier(n_estimators = num_rounds,\n",
    "                        objective= 'binary:logistic',\n",
    "                        eval_metric = 'logloss',\n",
    "                        seed = 2024)\n",
    "\n",
    "clf_XGB.fit(X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose = False)\n",
    "\n",
    "# The modification: Use xgboost's cross-validation\n",
    "try:\n",
    "  results = model_selection.cross_val_score(clf_XGB, X_train,y_train, cv = kfold)\n",
    "except AttributeError:\n",
    "  print(\"Using xgboost's cv instead of sklearn's\")\n",
    "  # create DMatrix for xgboost.cv\n",
    "  dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "  # parameters for xgboost.cv\n",
    "  params = clf_XGB.get_params()\n",
    "  # use xgboost's cross-validation\n",
    "  results = xgb.cv(params, dtrain, num_boost_round=num_rounds,\n",
    "                   nfold=kfold.n_splits, metrics='logloss',\n",
    "                   as_pandas=True, seed=2024)['test-logloss-mean'].tail(1).values[0]\n",
    "\n",
    "print ('xgBoost - CV train: %.2f' % results.mean() if hasattr(results, 'mean') else results) # print results based on type\n",
    "print ('xgBoost - train: %.2f' % metrics.accuracy_score(clf_XGB.predict(X_train), y_train))\n",
    "print ('xgBoost - test: %.2f' % metrics.accuracy_score(clf_XGB.predict(X_test), y_test))"
   ],
   "id": "8f67c9313ec33032"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. What is a difference between adaptive boosting, gradient boosting and eXtreme gradient boosting?\n",
    "2. Based on several model builds, compare the effectiveness of the xgboost model with the gradient boosting and adaptive boosting models.\n",
    "\n",
    "**Answers:**"
   ],
   "id": "55d6f5fc91d9dcfa"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. XGBoost implements parallel processing, it's optimized version of GradientBoosting, is faster and adds regularizations for better performance. On the other hand Gradient boosting is more flexible and can work with mre diverse tasks with parameter tuning and custom loss functions. And advantage of AdaBoost is that it's more intuitive and easy to explain, works good on small datasets, but also it is very sensitive to outliers.\n",
    "\n",
    "2. Based on several executions following conclusions can be made:\n",
    "XGBoost achieves the highest training accuracy because of reularization and tree-pruning. Adaptive boosting has the lowest training accuracy. XGBoost also achieves the highest testing accuracy thanks to recularisation. GradientBoosting also has good test accuracy but for large trees or many iterations will start to overfit. Adaptive Boosting has very good results on simple datasets, but when data has noise or complex patterns, this algorithm is not useful.\n",
    "XGBoost is the fastest and most consistent."
   ],
   "id": "4d0c2a8c4430cd15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 4: XGBoost using native interface\n",
    "**xgboost** has its own internal data structure **DMatrix** for input data. It is good practice to convert large dataset to **DMatrix** object to save preprocessing time."
   ],
   "id": "65104bde658059da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "xgtrain = xgb.DMatrix(X_train, label = y_train, missing = -999)\n",
    "xgtest = xgb.DMatrix(X_test, label = y_test, missing = -999)\n",
    "\n",
    "# Set xgboost params\n",
    "param = {'max_depth': 43,  # The maximum depth of each tree\n",
    "         'objective': 'binary:logistic',\n",
    "         'eval_metric': 'logloss'}\n",
    "\n",
    "clf_xgb_cv = xgb.cv(param, xgtrain, num_rounds,\n",
    "                    stratified = True,\n",
    "                    nfold = 5,\n",
    "                    early_stopping_rounds = 20)\n",
    "\n",
    "print ('Optimal number of trees / estimators is %i\\n' % clf_xgb_cv.shape[0])\n",
    "\n",
    "watchlist  = [(xgtest, 'test'), (xgtrain, 'train')]\n",
    "clf_xgb = xgb.train(param, xgtrain, clf_xgb_cv.shape[0], watchlist)\n",
    "\n",
    "# predict() function will produce the probability\n",
    "# so we'll use 0.5 cut-off to convert probability to class label\n",
    "y_train_pred = (clf_xgb.predict(xgtrain) >= 0.5).astype(int)\n",
    "y_test_pred = (clf_xgb.predict(xgtest) >= 0.5).astype(int)\n",
    "\n",
    "print ('\\nXGB - train: %.2f' % metrics.accuracy_score(y_train_pred, y_train))\n",
    "print ('XGB - test: %.2f' % metrics.accuracy_score(y_test_pred, y_test))"
   ],
   "id": "dade76ba29ccc3db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. Explain the meaning of the **max_depth** parameter.\n",
    "2. How does the value of the **max_depth** parameter affect the **xgboost** model? Check it out experimentally.\n",
    "\n",
    "**Answers:**"
   ],
   "id": "874f90dfc493d9cf"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. max_depth parameter shows the maximum depth of each desicion tree in the ensemble, which means how many splits can the tree go. The default value is 6.\n",
    "2. Smaller max_depth prevents overfitting, but limits the model's ability to learn complex patterns. Larger max_depth allows to find complex relations, but increases the risk of overfitting and slows down training and prediction time. At some point it just stops to get better and has the same result. Results:\n",
    "\n",
    "max_depth = 3:\n",
    "train: 0.75;\n",
    "test: 0.65;\n",
    "\n",
    "max_depth = 7:\n",
    "train: 0.77;\n",
    "test: 0.62;\n",
    "\n",
    "max_depth = 11:\n",
    "train: 0.78;\n",
    "test: 0.64;\n",
    "\n",
    "max_depth = 15:\n",
    "train: 0.78;\n",
    "test: 0.68;\n",
    "\n",
    "max_depth = 20:\n",
    "train: 0.78;\n",
    "test: 0.68;\n",
    "\n",
    "max_depth = 32:\n",
    "train: 0.78;\n",
    "test: 0.68;\n",
    "\n",
    "max_depth = 45:\n",
    "train: 0.78;\n",
    "test: 0.68;"
   ],
   "id": "b42d24050e11dec3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 5: Digit classification with gradient boosting\n",
    "Let's look at the digit classification to illustrate how the model performance improves with each iteration. The file *digit.csv* contains **20 000** observations, so the execution time of the following code can be even tens of seconds."
   ],
   "id": "3171e41f483cb4e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df= pd.read_csv('/content/drive/My Drive/Colab Notebooks/Lab10/digit.csv')\n",
    "\n",
    "X = df.iloc[:, 1:17].values\n",
    "y = df['letter'].values\n",
    "\n",
    "# Evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(n_splits = 5)\n",
    "num_trees = 10\n",
    "\n",
    "clf_GBT = GradientBoostingClassifier(n_estimators = num_trees, learning_rate = 0.1).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_GBT, X_train, y_train, cv = kfold)\n",
    "\n",
    "print ('Gradient boosting - train: %.2f' % metrics.accuracy_score(clf_GBT.predict(X_train), y_train))\n",
    "print ('Gradient boosting - test: %.2f' % metrics.accuracy_score(clf_GBT.predict(X_test), y_test))\n",
    "\n",
    "# Let's predict for the letter 'T' and understand how the prediction accuracy changes in each boosting iteration\n",
    "# X_valid = (2, 8, 3, 5, 1, 8, 13, 0, 6, 6, 10, 8, 0, 8, 0, 8)\n",
    "# for letter O\n",
    "# X_valid = (6, 13, 4, 7, 4, 6, 7, 6, 3, 10, 7, 9, 5, 9, 5, 8)\n",
    "# for letter M\n",
    "X_valid = (11, 15, 13, 9, 7, 13, 2, 6, 2, 12, 1, 9, 8, 1, 1, 8)\n",
    "\n",
    "\n",
    "print ('Predicted letter:', clf_GBT.predict([X_valid]))\n",
    "\n",
    "# Staged prediction will give the predicted probability for each boosting iteration\n",
    "stage_preds = list(clf_GBT.staged_predict_proba([X_valid]))\n",
    "final_preds = clf_GBT.predict_proba([X_valid])\n",
    "\n",
    "# Plot\n",
    "x = range(1, 27)\n",
    "label = np.unique(df['letter'])\n",
    "\n",
    "plt.figure(figsize = (10, 3))\n",
    "plt.subplot(131)\n",
    "plt.bar(x, stage_preds[0][0], align = 'center')\n",
    "plt.xticks(x, label)\n",
    "plt.xlabel('Letter')\n",
    "plt.ylabel('Prediction probability')\n",
    "plt.title('Iteration 1')\n",
    "plt.autoscale()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.bar(x, stage_preds[5][0], align = 'center')\n",
    "plt.xticks(x, label)\n",
    "plt.xlabel('Letter')\n",
    "plt.ylabel('Prediction probability')\n",
    "plt.title('Iteration 5')\n",
    "plt.autoscale()\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.bar(x, stage_preds[9][0], align = 'center')\n",
    "plt.xticks(x, label)\n",
    "plt.autoscale()\n",
    "plt.xlabel('Letter')\n",
    "plt.ylabel('Prediction probability')\n",
    "plt.title('Iteration 10')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2052315b79f171d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. Explain, how does the model behave with each successive iteration for prediction of the letter **T** in comparison with other letters?\n",
    "2. Run experiments and compare the prediction process for the letters **O** and **M**. What are the reasons for the differences?\n",
    "\n",
    "**Answers:**"
   ],
   "id": "744224b7679534b9"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. The model becomes more and more confident after each successive iteration. On the first iteration the probability for letter T is 0.25 and all the other letters are nearly equal to eachother. By 5th iteration the probability for T is more than 0.6 and by the 10th iteration prob of T is 0.8 dropping the prob of all other letters, because the sum of all of their probability is 0.2\n",
    "\n",
    "Gradient boosting - train: 0.74\n",
    "Gradient boosting - test: 0.74\n",
    "Predicted letter: ['T']\n",
    "\n",
    "\n",
    "2. *The modification is done in the same code cell and commented, (after the features for T are inserted)*.\n",
    "In case of the letter O, model is having difficulties to distinguish, because there are several letters alike O, for example G and Q. In first iteration the highest probability has the letter G almost 0.07, letters U and I are also high about 0.05, letter O is near 0.04. All the other letters are below 0.04  By 5th iteration the highest is again G, with 0.12 probability, next one is O with 0.11. All the other letters are below. By 10th iteration finally the model is more confident for letter O (0.14) than letter G (0.13). *The values of probabilities are not exact, but approximately by comparison and visual sight.*\n",
    "\n",
    "Gradient boosting - train: 0.75\n",
    "Gradient boosting - test: 0.72\n",
    "Predicted letter: ['O']\n",
    "\n",
    "\n",
    "2. In case of the letter M, starting from the first iteration there is a strong distinguishment. Letter M has probability more than 0.3, letter N has prob 0.05 and all the other letters are below 0.04. On the 5th iteration probability of letter M is 0.6 and on the 10th iteration it's almost 0.8.\n",
    "\n",
    "Gradient boosting - train: 0.75\n",
    "Gradient boosting - test: 0.75\n",
    "Predicted letter: ['M']\n",
    "\n"
   ],
   "id": "a21bbdaaefa0b07e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Hyperparameter tuning\n",
    "One of the primary objective and challenge in machine learning process is improving the performance score, based on data patterns and observed evidence. To achieve this objective, almost all machine learning algorithms have specific set of parameters that needs to estimate from dataset which will maximize the performance score.\n",
    "\n",
    "The best way to choose good hyperparameters is through trial and error of all possible combination of parameter values. Scikit-learn provide **GridSearch** and **RandomSearch** functions to facilitate automatic and reproducible approach for hyperparameter tuning.\n",
    "## Exercise 6: GridSearch\n",
    "The execution time for the following code will be approx. 15 minutes. Wait patiently until it's over."
   ],
   "id": "da3ffd6123392a91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "seed = 2024\n",
    "\n",
    "# Read the data in\n",
    "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Lab10/Diabetes.csv')\n",
    "\n",
    "X = df.iloc[:,:8].values     # Independent variables\n",
    "y = df['class'].values       # Dependent variable\n",
    "\n",
    "# Evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(n_splits = 5)\n",
    "num_trees = 100\n",
    "\n",
    "clf_rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 250, 500, 750, 1000],\n",
    "    'criterion':  ['gini', 'entropy'],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 1, 3, 5, 7, 9]\n",
    "}\n",
    "\n",
    "# Setting verbose = 10 will print the progress for every 10 task completion\n",
    "grid = GridSearchCV(clf_rf, rf_params,\n",
    "                    scoring = 'roc_auc',\n",
    "                    cv = kfold,\n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print ('Best Parameters:', grid.best_params_)\n",
    "\n",
    "base_model = model_selection.cross_val_score(clf_rf, X_train, y_train, cv = kfold)\n",
    "gs_results = model_selection.cross_val_score(grid.best_estimator_, X_train, y_train, cv = kfold)\n",
    "print ('\\nAccuracy - base model train CV:', base_model.mean())\n",
    "print ('Accuracy - GridSearch train CV:', gs_results.mean())\n",
    "print ('Accuracy - train:', metrics.accuracy_score(grid.best_estimator_.predict(X_train), y_train))\n",
    "print ('Accuracy - test:', metrics.accuracy_score(grid.best_estimator_.predict(X_test), y_test))"
   ],
   "id": "75292d846208e775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. What is the idea behind the **GridSearch** method?\n",
    "2. What are the advantages and disadvantages of the **GridSearch** method?\n",
    "\n",
    "**Answers:**"
   ],
   "id": "bd7f80586bdcd458"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. GridSearch is a method to perform hyperparameter tuning. It's goal is to find combination of hyperparameters that will maximise model's performance on a given dataset. So models are built for all possible combinations, then with cross validation is chosen the best combination.\n",
    "2. Advantages of GridSearch is that it evaluates all possible combinations of hyperparameters, so nothing will be missed and everything is automated to reduce chance of mannual effort. The biggest disadvantage of Grid Search is that it's computatuionally expensive and time consuming. Adding more hyperparameters will increase the grid exponentially, so in high dimentional parameter spaces this method is not practical.\n",
    "\n",
    "Result of the code above: Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
    "Best Parameters: {'criterion': 'entropy', 'max_depth': 9, 'max_features': 'sqrt', 'n_estimators': 100}\n",
    "\n",
    "Accuracy - base model train CV: 0.7578920041536863\n",
    "Accuracy - GridSearch train CV: 0.7616649359640014\n",
    "Accuracy - train: 0.9757914338919925\n",
    "Accuracy - test: 0.7489177489177489"
   ],
   "id": "1b4599fe3186e91f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 7: RandomSearch\n",
    "The execution time in this case will be about 1 minute."
   ],
   "id": "1d985f4f2a0f8576"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# Specify parameters and distributions to sample from\n",
    "param_dist = {'n_estimators': sp_randint(100, 1000),\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "              'max_depth': [None, 1, 3, 5, 7, 9]\n",
    "             }\n",
    "\n",
    "# Run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf_rf, param_distributions = param_dist,\n",
    "                                   cv = kfold,\n",
    "                                   n_iter = n_iter_search,\n",
    "                                   verbose = 1,\n",
    "                                   n_jobs = -1,\n",
    "                                   random_state = seed)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Report(random_search.cv_results_)\n",
    "print ('Best parameters:', random_search.best_params_)\n",
    "results = model_selection.cross_val_score(random_search.best_estimator_, X_train, y_train, cv = kfold)\n",
    "print ('\\nAccuracy - train CV:', results.mean())\n",
    "print ('Accuracy - train:', metrics.accuracy_score(random_search.best_estimator_.predict(X_train), y_train))\n",
    "print ('Accuracy - test:', metrics.accuracy_score(random_search.best_estimator_.predict(X_test), y_test))"
   ],
   "id": "199c20467722aab6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "1. What is the idea behind the **RandomSearch** method?\n",
    "2. What are the advantages and disadvantages of the **RandomSearch** method?\n",
    "3. Compare the results of the **GridSearch** and **RandomSearch** methods in terms of the accuracy of the best model and the time taken to build it.\n",
    "\n",
    "**Answers:**"
   ],
   "id": "65611d00b31d3433"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "1. RandomSearch is hyperparameter tuning metod, which tries random combinations in a range of values of given parameters. The idea is to explore parameter space more efficiently by focusing on a subset, instead of trying every possible combination.\n",
    "2. Advantages: Useful for high dimensional parameter spaces, because numerical parameters can be specified as a range, unlike GridSearch, where values are fixed. The number of iterations also can be controlled. Disadvantages: Since it doesn't test all compinations, there's no certainity that RandomSeach will find the best set of hyperparameters. Also the results can very between runs because of the random sampling\n",
    "3. Since RandomSearch CV accuracy is slightly higher, it means it found a bit better combination of hyperparameters. However, RandomSearch has training accuracy 1 which may indicate overfitting. Grid search took 17X more time\n",
    "\n",
    "GridSearch train CV: 0.758;\n",
    "RandomSearch train CV: 0.765;\n",
    "\n",
    "GridSearch train: 0.976;\n",
    "RandomSearch train: 1;\n",
    "\n",
    "GridSearch test: 0.749;\n",
    "RandomSeach test: 0.775;\n"
   ],
   "id": "c8239298f22e5afb"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
